{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paper-d book.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsoy/AutoEncoder-SSIM-for-unsupervised-anomaly-detection-/blob/master/paper_d_book.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EXoS-8gwxlx"
      },
      "source": [
        "# Convolutional and Variational Autoencoders\n",
        "\n",
        "Typically, autoencoders don't work well with images unless they are very small. However, a convolutional neural network (CNN) works much better than a feedforward dense network with large color images. We demonstrate a convolutional autoencoder with a code example. We also demonstrate a variational encoder with a code example. Variational autoencoders are quite different from the other autoencoders in two ways. They are probabilistic because their outputs are partly determined by chance even after training. Most importantly, they are generative. That is, they can generate new instances that look like they were sampled from the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNlEEbBuRoOb"
      },
      "source": [
        "# Import **tensorflow** library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLDqhpblZW9P"
      },
      "source": [
        "Import library and alias it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsx7Y3HqRos6"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7UC2JNPw3pY"
      },
      "source": [
        "# GPU Hardware Accelerator\n",
        "\n",
        "To vastly speed up processing, we can use the GPU available from the Google Colab cloud service. Colab provides a free Tesla K80 GPU of about 12 GB. Itâ€™s very easy to enable the GPU in a Colab notebook:\n",
        "\n",
        "1.\tclick **Runtime** in the top left menu\n",
        "2.\tclick **Change runtime** type from the drop-down menu\n",
        "3.\tchoose **GPU** from the Hardware accelerator drop-down menu\n",
        "4.\tclick **SAVE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDNFpWCTw7L4"
      },
      "source": [
        "Verify that GPU is active:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck2w8yEvwqpp"
      },
      "source": [
        "tf.__version__, tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlK8MFMaHZVz"
      },
      "source": [
        "# Convolutional Autoencoders\n",
        "\n",
        "Autoencoders won't work well with images unless they are very small. However, a convolutional neural network (CNN) works much better than feedforward dense networks with large color images. So let's use a dataset with large color images.\n",
        "\n",
        "Resource:\n",
        "\n",
        "https://codelabs.developers.google.com/codelabs/tensorflow-lab5-compleximages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPy4XdUdiBK7"
      },
      "source": [
        "## Horses or Humans Dataset\n",
        "\n",
        "The **horses_or_humans** TFDS is a set of images of horses and humans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD_J7d7cAQio"
      },
      "source": [
        "Get data for inspection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KUceTiAiBQI"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "data, hh_info = tfds.load(\n",
        "    'horses_or_humans', with_info=True,\n",
        "    split='train', try_gcs=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POkaanOliSM7"
      },
      "source": [
        "Display metadata:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo6y3RZHiShL"
      },
      "source": [
        "hh_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qcy5ePvWKJQ"
      },
      "source": [
        "Get metadata:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhgKro7CWKQD"
      },
      "source": [
        "class_labels = hh_info.features['label'].names\n",
        "num_classes = hh_info.features['label'].num_classes\n",
        "class_labels, num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LMQRBiNjARd"
      },
      "source": [
        "## Show Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Rwr8L9jcsV"
      },
      "source": [
        "Use show_examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVP1jslbjAXX"
      },
      "source": [
        "fig = tfds.show_examples(data, hh_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d70swkSrjVrd"
      },
      "source": [
        "Convert to a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcfczDHwjVvt"
      },
      "source": [
        "tfds.as_dataframe(data.take(4), hh_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMQNzSjGV1ml"
      },
      "source": [
        "Display an image manually:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJgDfNN0XGxI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for element in data.take(1):\n",
        "  plt.imshow(element['image'])\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOMqHf5JZiTY"
      },
      "source": [
        "Display a grid of images manually:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nneVJlV0YzsX"
      },
      "source": [
        "img, lbl = [], []\n",
        "for element in data.take(9):\n",
        "  img.append(element['image'])\n",
        "  lbl.append(element['label'].numpy())\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows+1):\n",
        "  fig.add_subplot(rows, columns, i)\n",
        "  plt.imshow(img[i-1])\n",
        "  plt.title(class_labels[lbl[i-1]])\n",
        "  plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpkxyu0lbRja"
      },
      "source": [
        "Create a set of images and labels. Plot the images and labels in a grid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5V2tLGljvD2"
      },
      "source": [
        "## Get Data for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znVTcIM1a9ye"
      },
      "source": [
        "Now that we have inspected the dataset and know its essence, we can load it for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z921XqpSHZcT"
      },
      "source": [
        "(x_train_img, _), (x_test_img, _) = tfds.as_numpy(\n",
        "    tfds.load(\n",
        "        'horses_or_humans', split=['train','test'],\n",
        "        batch_size=-1, as_supervised=True,\n",
        "        try_gcs=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QdJ134IhzZ1"
      },
      "source": [
        "Since autoencoders are unsupervised models, we don't need the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vm40rRpBAU_"
      },
      "source": [
        "Get split information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-Mm9k62BAdf"
      },
      "source": [
        "len(x_train_img), len(x_test_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaKO8NL7W6Ul"
      },
      "source": [
        "## Inpsect Shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk1fpkFgelgz"
      },
      "source": [
        "Display shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDPyrvL0W6ZW"
      },
      "source": [
        "x_train_img.shape, x_test_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G80vViMf_gPn"
      },
      "source": [
        "Check shapes of images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3r7qztq_gT1"
      },
      "source": [
        "for element in range(10):\n",
        "  print (x_train_img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPKT0UFj_8dh"
      },
      "source": [
        "Since images are the same shape, we don't have to resize for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AormAu1YPmFv"
      },
      "source": [
        "# Scale Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQbUvXdLbG-m"
      },
      "source": [
        "Scale images by dividing by the number of pixels in an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dBkMfJZOOq8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train, x_test = x_train_img.astype(np.float32) / 255,\\\n",
        "                  x_test_img.astype(np.float32) / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUvxdtS_gZ5x"
      },
      "source": [
        "Inspect a vector from the train set to verify scaling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8Vcvx0ngaAX"
      },
      "source": [
        "x_train_img[0][0][0], x_train[0][0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGNa47QERqfv"
      },
      "source": [
        "## Clear Previous Models and Generate Seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFthc1iqbNbl"
      },
      "source": [
        "Clear previous model and generate a seed for reproducibility of results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbqASD0HRqj-"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea9KwTlPfsWr"
      },
      "source": [
        "Get input shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSlhbI_XW9z0"
      },
      "source": [
        "hh_shape = hh_info.features['image'].shape\n",
        "hh_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSxp7J8kRinO"
      },
      "source": [
        "## Create Encoder and Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egO-XdSde3eQ"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BQKCBOpRitS"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D,\\\n",
        "Dense, Flatten, Input, Conv2DTranspose\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyjKGh4Oai_A"
      },
      "source": [
        "Create encoder model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Y1idy-ajOX"
      },
      "source": [
        "conv_encoder = Sequential([\n",
        "  Input(shape=hh_shape),\n",
        "  Conv2D(16, kernel_size=3, padding='SAME', activation='selu'),\n",
        "  MaxPool2D(pool_size=2),\n",
        "  Conv2D(32, kernel_size=3, padding='SAME', activation='selu'),\n",
        "  MaxPool2D(pool_size=2),\n",
        "  Conv2D(64, kernel_size=3, padding='SAME', activation='selu'),\n",
        "  MaxPool2D(pool_size=2)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeAhXf9YWwUp"
      },
      "source": [
        "The encoder is composed of convolutional layers and pooling layers. It reduces the spatial dimensionality of the inputs (height and width) while increasing the depth (number of feature maps)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlcq_TVeWg36"
      },
      "source": [
        "Create the decoder model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owrL-XsGWg9x"
      },
      "source": [
        "conv_decoder = Sequential([\n",
        "  Conv2DTranspose(32, kernel_size=3, strides=2, padding='VALID',\n",
        "                  activation='selu'),\n",
        "  Conv2DTranspose(16, kernel_size=3, strides=2, padding='SAME',\n",
        "                  activation='selu'),\n",
        "  Conv2DTranspose(3, kernel_size=3, strides=2, padding='SAME',\n",
        "                  activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Aa7yZ0XR6i"
      },
      "source": [
        "The decoder must do the reverse of the encoder by upscaling images and reducing their depth back to original dimensions. We use Conv2DTranspose for this purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOJ6GVq4cXtF"
      },
      "source": [
        "## Create Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agPgya_pbY73"
      },
      "source": [
        "Create the autoencoder from the convolutional encoder and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZEEbdVrfRGf"
      },
      "source": [
        "conv_ae = Sequential([conv_encoder, conv_decoder])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIIBMxC-fWKM"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e-BFybMfaPE"
      },
      "source": [
        "Create function for metric:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9yCZokrcXxT"
      },
      "source": [
        "def rounded_accuracy(y_true, y_pred):\n",
        "    return tf.keras.metrics.binary_accuracy(\n",
        "        tf.round(y_true), tf.round(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk1Bm_lZfhMs"
      },
      "source": [
        "Compile model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI3CDIzmfWOt"
      },
      "source": [
        "conv_ae.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.SGD(lr=1.0),\n",
        "    metrics=[rounded_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e856DSsLa-7Z"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvfQRe10bhYB"
      },
      "source": [
        "Train for five epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE6a0-Oda-_a"
      },
      "source": [
        "cae_history = conv_ae.fit(\n",
        "    x_train, x_train, epochs=5,\n",
        "    validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaaRRutyWMH7"
      },
      "source": [
        "## Visualize Training Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwj77C5lWQhR"
      },
      "source": [
        "Create a visualization function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm8bptgoWMOh"
      },
      "source": [
        "def viz_history(training_history):\n",
        "  loss = training_history.history['loss']\n",
        "  val_loss = training_history.history['val_loss']\n",
        "  accuracy = training_history.history['rounded_accuracy']\n",
        "  val_accuracy = training_history.history['val_rounded_accuracy']\n",
        "  plt.figure(figsize=(14, 4))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(loss, label='Training set')\n",
        "  plt.plot(val_loss, label='Test set', linestyle='--')\n",
        "  plt.legend()\n",
        "  plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(accuracy, label='Training set')\n",
        "  plt.plot(val_accuracy, label='Test set', linestyle='--')\n",
        "  plt.legend()\n",
        "  plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY2dliQ4WgAa"
      },
      "source": [
        "Invoke:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsneU6h0WgLS"
      },
      "source": [
        "viz_history(cae_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "556_ZiqDfyRu"
      },
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKDuBoHQA4PS"
      },
      "source": [
        "Create function to show reconstructions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3GxEVodfyWA"
      },
      "source": [
        "def show_reconstructions(model, images, n_images, reshape=False):\n",
        "  reconstructions = model.predict(images[:n_images])\n",
        "  if reshape:\n",
        "    reconstructions = tf.squeeze(reconstructions) # drop '1' dimension\n",
        "  fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
        "  for image_index in range(n_images):\n",
        "    plt.subplot(2, n_images, 1 + image_index)\n",
        "    plot_image(images[image_index])\n",
        "    plt.subplot(2, n_images, 1 + n_images + image_index)\n",
        "    plot_image(reconstructions[image_index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkLwqydZfz4l"
      },
      "source": [
        "Create function to show an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeaLTI5Ifz9x"
      },
      "source": [
        "def plot_image(image):\n",
        "  plt.imshow(image, cmap='binary')\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h34b2y4KBF6B"
      },
      "source": [
        "Show images and reconstructions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8FWVDRvBF_z"
      },
      "source": [
        "show_reconstructions(conv_ae, x_test, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOQuivNwhA1p"
      },
      "source": [
        "# Variational Autoencoders\n",
        "\n",
        "Variational autoencoders are quite different from the other autoencoders in two ways. They are probabilistic because their outputs are partly determined by chance even after training. Most importantly, they are generative. That is, they can generate new instances that look like they were sampled from the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "502vb94LiBGz"
      },
      "source": [
        "## How Variational Autoencoders Work\n",
        "\n",
        "Instead of directly producing a coding (latent representation) for a given input, the endocder produces a mean coding $u$ and a standard deviation $\\sigma$. The actual coding is then sampled randomly from a Gaussian distribution with mean $u$ and standard deviatino $\\sigma$. The decoder then decodes the sampled coding normally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElZiZ7_ki6Tt"
      },
      "source": [
        "# Wonderful Resources\n",
        "\n",
        "https://blog.tensorflow.org/2019/03/variational-autoencoders-with.html\n",
        "\n",
        "https://www.tensorflow.org/probability/examples/Probabilistic_Layers_VAE\n",
        "\n",
        "https://www.tensorflow.org/probability/overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu1TcrcmsPGA"
      },
      "source": [
        "## Load Fashion-MNIST Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P71vHGf5b1Dw"
      },
      "source": [
        "Load train and test feature images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJJCTV5XsPVI"
      },
      "source": [
        "(x_train_fm, _), (x_test_fm, _) = tfds.as_numpy(\n",
        "    tfds.load('fashion_mnist', split=['train','test'],\n",
        "              batch_size=-1, as_supervised=True,\n",
        "              try_gcs=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEZn-y71sgVH"
      },
      "source": [
        "## Get Input Shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UTBRxXLtR-h"
      },
      "source": [
        "Get shape of images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPykma9Osgb_"
      },
      "source": [
        "x_train_fm.shape, x_test_fm.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqZqdfzotUZR"
      },
      "source": [
        "Get input shape for encoder and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS2OnCHZtUdy"
      },
      "source": [
        "fmnist_shape = x_train_fm.shape[1:]\n",
        "fmnist_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77egSdtcsy08"
      },
      "source": [
        "## Scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr1QNMEqcFe-"
      },
      "source": [
        "Scale feature images by dividing by the number of pixels in each image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eE2__yKsy5n"
      },
      "source": [
        "x_train_fds, x_test_fds = x_train_fm.astype(np.float32) / 255,\\\n",
        "                          x_test_fm.astype(np.float32) / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCb6xmsymQ2r"
      },
      "source": [
        "## Create a Custom Layer to Sample Codings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pch8fzeAcNxY"
      },
      "source": [
        "The sampling layer takes two inputs: mean $u$ and log variance $y$. It uses function tf.random.normal() to sample a random vector of the same shape as $y$ from the normal distribution with mean 0 ($u$ = 0) and standard deviation 1 ($\\sigma$ = 1), multiplies it by $exp(y/2$), adds $u$, and returns the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2FKhCXRmQ9A"
      },
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    mean, log_var = inputs\n",
        "    return tf.random.normal(tf.shape(log_var)) *\\\n",
        "           tf.math.exp(log_var / 2) + mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgFc7De98f5j"
      },
      "source": [
        "## Clear Models and Generate Seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR-JqnaWcRTo"
      },
      "source": [
        "Clear previous models and seed for reproducibility of results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xODmdVU08f9w"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq27onyMrcGo"
      },
      "source": [
        "## Create the Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3hTyMHgcXKw"
      },
      "source": [
        "Use the Functional API because the model isn't entirely sequential:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrnkSQthu62C"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Reshape\n",
        "\n",
        "codings_size = 10\n",
        "\n",
        "inputs = Input(shape=fmnist_shape)\n",
        "z = Flatten()(inputs)\n",
        "z = Dense(128, activation='relu')(z)\n",
        "z = Dense(64, activation='relu')(z)\n",
        "z = Dense(32, activation='relu')(z)\n",
        "codings_mean = Dense(codings_size)(z)\n",
        "codings_log_var = Dense(codings_size)(z)\n",
        "codings = Sampling()([codings_mean, codings_log_var])\n",
        "variational_encoder = Model(\n",
        "    inputs=[inputs],\n",
        "    outputs=[codings_mean, codings_log_var, codings])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4mFs4KHwePO"
      },
      "source": [
        "The Dense layers output codings_mean $u$ and codings_log_var $y$ that both have the same inputs (i.e., the outputs of the second Dense layer). Both codings_mean and codings_log_var are then passed to the Sampling layer. The variational_encoder has three outputs, namely, codings_mean, codings_log_var, and codings. But, we only use the **codings** output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu8CWrUHwIvN"
      },
      "source": [
        "## Create the Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfu9Av7LdKHZ"
      },
      "source": [
        "Create the variational decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMxDxL1AvZrz"
      },
      "source": [
        "decoder_inputs = Input(shape=[codings_size])\n",
        "x = Dense(32, activation='relu')(decoder_inputs)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(28 * 28, activation='sigmoid')(x)\n",
        "outputs = Reshape(fmnist_shape)(x)\n",
        "variational_decoder = Model(\n",
        "    inputs=[decoder_inputs], outputs=[outputs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JSFdyeIxlc1"
      },
      "source": [
        "We could have used the Sequential API instead of the Functional API since it is really just a simple stack of layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP8q-u8Ixxut"
      },
      "source": [
        "## Build the Variational Encoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDLyUaG2dYcb"
      },
      "source": [
        "Create the autoencoder from the variational encoder and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDf-xyuNgn2Z"
      },
      "source": [
        "_, _, codings = variational_encoder(inputs)\n",
        "reconstructions = variational_decoder(codings)\n",
        "variational_ae = Model(\n",
        "    inputs=[inputs], outputs=[reconstructions])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlF5WsBQx7x2"
      },
      "source": [
        "We ignore the first two outputs because we only need the codings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpkMxGxByJpl"
      },
      "source": [
        "## Add Latent Loss and Reconstruction Loss to the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiN-pMxVdenY"
      },
      "source": [
        "Compute latent loss as 1 plus codings_log_var minus the exponential of codings_log_var minus the square of codings_mean. Multiply this result by -0.5. Compute reconstruction loss as mean loss over all instances in the batch and divide by 784 to ensure appropriate scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBpbyAPOgn4n"
      },
      "source": [
        "latent_loss = -0.5 * tf.math.reduce_sum(\n",
        "    1 + codings_log_var - tf.math.exp(codings_log_var) -\\\n",
        "    tf.math.square(codings_mean), axis=-1)\n",
        "\n",
        "variational_ae.add_loss(\n",
        "    tf.math.reduce_mean(latent_loss) / 784.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGuKQ29a8pS_"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwgTBYBpdkjB"
      },
      "source": [
        "Compile with **binary crossentropy** and **rmsprop** loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS6HBenr8paH"
      },
      "source": [
        "variational_ae.compile(\n",
        "    loss='binary_crossentropy', optimizer='rmsprop',\n",
        "    metrics=[rounded_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnLmx1sn8uCq"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNEHvJsTdq4r"
      },
      "source": [
        "Train for ten epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LKC4JACyRg-"
      },
      "source": [
        "vae_history = variational_ae.fit(\n",
        "    x_train_fds, x_train_fds, epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_data=(x_test_fds, x_test_fds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPGTXcsZfUzQ"
      },
      "source": [
        "Visualize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3rmqENQfU4n"
      },
      "source": [
        "viz_history(vae_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrIUR16Q2sLc"
      },
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiBlPYyj6gHH"
      },
      "source": [
        "Inspect the shape of the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NZfBNDg6gLo"
      },
      "source": [
        "x_test_fds.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_0XCwn96cn2"
      },
      "source": [
        "Remove the '1' dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5vrbnT16cuX"
      },
      "source": [
        "x_test_fds_imgs = tf.squeeze(x_test_fds)\n",
        "x_test_fds_imgs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Uo34BuWAjpu"
      },
      "source": [
        "Visualize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtbnFtrmyRjo"
      },
      "source": [
        "show_reconstructions(\n",
        "    variational_ae, x_test_fds_imgs, 5, reshape=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v37NyMkBrL-"
      },
      "source": [
        "# Generate Fashion-MNIST Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSNwM3fGB6w-"
      },
      "source": [
        "Create a plotting function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzIZHa8XBrRL"
      },
      "source": [
        "def plot_multiple_images(images, n_cols=None):\n",
        "  n_cols = n_cols or len(images)\n",
        "  n_rows = (len(images) - 1) // n_cols + 1\n",
        "  if images.shape[-1] == 1:\n",
        "    images = np.squeeze(images, axis=-1)\n",
        "  plt.figure(figsize=(n_cols, n_rows))\n",
        "  for index, image in enumerate(images):\n",
        "    plt.subplot(n_rows, n_cols, index + 1)\n",
        "    plt.imshow(image, cmap='binary')\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mA3xZ0yB9Ym"
      },
      "source": [
        "Generate a few random codings, decode them, and plot the resulting images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ75OYPpCBBW"
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "codings = tf.random.normal(shape=[12, codings_size])\n",
        "images = variational_decoder(codings).numpy()\n",
        "plot_multiple_images(images, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJKVv-0TCS5q"
      },
      "source": [
        "Notice that we used the **variational_decoder**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8iud6_5Cawv"
      },
      "source": [
        "Perform semantic interpolation between these images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRXCmMrBCa5U"
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "codings_grid = tf.reshape(codings, [1, 3, 4, codings_size])\n",
        "larger_grid = tf.image.resize(codings_grid, size=[5, 7])\n",
        "interpolated_codings = tf.reshape(\n",
        "    larger_grid, [-1, codings_size])\n",
        "images = variational_decoder(interpolated_codings).numpy()\n",
        "images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-iOJhOqDD_3"
      },
      "source": [
        "To display images, remove the '1' dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJk94yECDEGE"
      },
      "source": [
        "images = tf.squeeze(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqxwsU3FDBSx"
      },
      "source": [
        "Visualize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVGypVVJDBeI"
      },
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "for index, image in enumerate(images):\n",
        "  plt.subplot(5, 7, index + 1)\n",
        "  if index%7%2==0 and index//7%2==0:\n",
        "    plt.gca().get_xaxis().set_visible(False)\n",
        "    plt.gca().get_yaxis().set_visible(False)\n",
        "  else:\n",
        "    plt.axis('off')\n",
        "  plt.imshow(image, cmap='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrxKVDGafcZ3"
      },
      "source": [
        "# TensorFlow Probability Layers\n",
        "\n",
        "Fit a Variational Autoencoder using TensorFlow Probability Layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WXp94Ijfcm2"
      },
      "source": [
        "## Load and Process Train and Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNTNTDljjcwO"
      },
      "source": [
        "Load Fashion-MNIST, preprocess, and map the input pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxW78CI7lpWx"
      },
      "source": [
        "fmnist, fmnist_info = tfds.load(\n",
        "    name='fashion_mnist', try_gcs=True,\n",
        "    with_info=True, as_supervised=False)\n",
        "\n",
        "def _preprocess(sample):\n",
        "  image = tf.cast(sample['image'], tf.float32) / 255.  # Scale to unit interval.\n",
        "  image = image < tf.random.uniform(tf.shape(image))   # Randomly binarize.\n",
        "  return image, image\n",
        "\n",
        "auto = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE, SHUFFLE_SIZE = 256, int(10e3)\n",
        "\n",
        "train_tpl = (fmnist['train']\n",
        "             .map(_preprocess)\n",
        "             .batch(BATCH_SIZE)\n",
        "             .prefetch(auto)\n",
        "             .shuffle(SHUFFLE_SIZE))\n",
        "test_tpl = (fmnist['test']\n",
        "            .map(_preprocess)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .prefetch(auto))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHArtsHQkd8f"
      },
      "source": [
        "## Inspect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPtY4K9Ej15o"
      },
      "source": [
        "Inspect a slice from a batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi3WIPSCt7Li"
      },
      "source": [
        "for example in train_tpl.take(1):\n",
        "  print (example[0][0][0][0:15])\n",
        "  print (example[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE7_DHiMkavY"
      },
      "source": [
        "Inspect the shape of a batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxxYBlHNveBT"
      },
      "source": [
        "for row in train_tpl.take(1):\n",
        "  print (row[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIG7YDzskswn"
      },
      "source": [
        "## Create the TPL VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4e5BxCSmBNS"
      },
      "source": [
        "Create a TFP independent Gaussian distribution with no learned parameters. The latent variable, z has 16 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeJxs0_Tfct6"
      },
      "source": [
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfd = tfp.distributions\n",
        "encoded_size = 16\n",
        "prior = tfd.Independent(\n",
        "    tfd.Normal(\n",
        "        loc=tf.zeros(encoded_size), scale=1),\n",
        "        reinterpreted_batch_ndims=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QglMGMTlmaK5"
      },
      "source": [
        "Get the input shape and base depth:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU2BzPLBpZuU"
      },
      "source": [
        "input_shape = fmnist_info.features['image'].shape\n",
        "base_depth = 32\n",
        "input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1569vfDmwWh"
      },
      "source": [
        "Assign aliases for convenience:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bqd880ukbNM"
      },
      "source": [
        "tfpl = tfp.layers\n",
        "tfd = tfp.distributions\n",
        "leaky = tf.nn.leaky_relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9EH3gJImSjp"
      },
      "source": [
        "Create an encoder with a full-covariance Gaussian distribution with mean and covariance matrices parameterized by the output of a neural network. TFP layers enables contruction of this complex encoder very easy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzPcvQdAhCa5"
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "from tensorflow.keras.layers import InputLayer, Lambda\n",
        "\n",
        "encoder = Sequential([\n",
        "  InputLayer(input_shape=input_shape),\n",
        "  Lambda(lambda x: tf.cast(x, tf.float32) - 0.5),\n",
        "  Conv2D(base_depth, 5, strides=1, padding='same',\n",
        "         activation=leaky),\n",
        "  Conv2D(base_depth, 5, strides=2, padding='same',\n",
        "         activation=leaky),\n",
        "  Conv2D(base_depth * 2, 5, strides=1,\n",
        "         padding='same', activation=leaky),\n",
        "  Conv2D(base_depth * 2, 5, strides=2, padding='same',\n",
        "         activation=leaky),\n",
        "  Conv2D(4 * encoded_size, 7, strides=1, padding='valid',\n",
        "         activation=leaky),\n",
        "  Flatten(),\n",
        "  Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
        "        activation=None),\n",
        "  tfpl.MultivariateNormalTriL(\n",
        "      encoded_size,\n",
        "      activity_regularizer=tfpl.KLDivergenceRegularizer(\n",
        "          prior, weight=1.0))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmIbwq-GngnE"
      },
      "source": [
        "The encoder is a normal Keras Sequential model with convolutions and dense layers. But the output is passed to a TFP Layer (MultivariateNormalTril()), which transparently splits the activations from the final Dense layer into the parts needed to specify both the mean and the (lower triangular) covariance matrix (the parameters of a Multivariate Normal). We use the tfpl helper MultivariateNormalTriL.params_size(encoded_size) to make the Dense layer output the correct number of activations (i.e., the distributionâ€™s parameters). We also contribute a regularization term to the final loss. Specifically, we add the KL divergence between the encoder and the prior to the loss, which is the KL term in the ELBO that we described above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTekbDe8rOpf"
      },
      "source": [
        "Create the decoder as a pixel-independent Bernoulli distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcVsrJ9nhCdX"
      },
      "source": [
        "decoder = Sequential([\n",
        "  InputLayer(input_shape=[encoded_size]),\n",
        "  Reshape([1, 1, encoded_size]),\n",
        "  Conv2DTranspose(2 * base_depth, 7, strides=1,\n",
        "                  padding='valid', activation=leaky),\n",
        "  Conv2DTranspose(2 * base_depth, 5, strides=1,\n",
        "                  padding='same', activation=leaky),\n",
        "  Conv2DTranspose(2 * base_depth, 5, strides=2,\n",
        "                  padding='same', activation=leaky),\n",
        "  Conv2DTranspose(base_depth, 5, strides=1,\n",
        "                  padding='same', activation=leaky),\n",
        "  Conv2DTranspose(base_depth, 5, strides=2,\n",
        "                  padding='same', activation=leaky),\n",
        "  Conv2DTranspose(base_depth, 5, strides=1,\n",
        "                  padding='same', activation=leaky),\n",
        "  Conv2D(filters=1, kernel_size=5, strides=1,\n",
        "         padding='same', activation=None),\n",
        "  Flatten(),\n",
        "  tfpl.IndependentBernoulli(\n",
        "      input_shape, tfd.Bernoulli.logits)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViyaaFZPrbuK"
      },
      "source": [
        "The form here is essentially the same as the encoder, but now we are using transposed convolutions to take our latent representation (a 16-dimensional vector) and turn it into a 28 x 28 x 1 tensor. That final layer parameterizes the pixel-independent Bernoulli distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4GgHpJzruLn"
      },
      "source": [
        "Build the full model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MTyHDbdk1WQ"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "\n",
        "tpl_vae = Model(inputs=encoder.inputs,\n",
        "                outputs=decoder(encoder.outputs[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN2MtMbfsZ4N"
      },
      "source": [
        "Check that the inputs are created appropriately:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5ONzEuZxC1N"
      },
      "source": [
        "encoder.inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCsqAeu6rxVo"
      },
      "source": [
        "Compile with appropriate loss and train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfraFiEBpoM9"
      },
      "source": [
        "lr = 1e-3\n",
        "lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6xxJ69Xk1Y3"
      },
      "source": [
        "negloglik = lambda x, rv_x: -rv_x.log_prob(x)\n",
        "\n",
        "tpl_vae.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=lr),\n",
        "    loss=negloglik)\n",
        "\n",
        "_ = tpl_vae.fit(train_tpl, epochs=15,\n",
        "                validation_data=test_tpl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5En7U1VMtmLE"
      },
      "source": [
        "Our model is just a Keras Model where the outputs are defined as the composition of the encoder and the decoder. Since the encoder already added the KL term to the loss, we need to specify only the reconstruction loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxV722cxvxEB"
      },
      "source": [
        "## Efficacy Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls-pnaHOn94j"
      },
      "source": [
        "# We'll just examine ten random digits.\n",
        "x = next(iter(test_tpl))[0][:10]\n",
        "xhat = tpl_vae(x)\n",
        "assert isinstance(xhat, tfd.Distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOzAM6Sn-qR"
      },
      "source": [
        "def display_imgs(x, y=None):\n",
        "  if not isinstance(x, (np.ndarray, np.generic)):\n",
        "    x = np.array(x)\n",
        "  plt.ioff()\n",
        "  n = x.shape[0]\n",
        "  fig, axs = plt.subplots(1, n, figsize=(n, 1))\n",
        "  if y is not None:\n",
        "    fig.suptitle(np.argmax(y, axis=1))\n",
        "  for i in range(n):\n",
        "    axs.flat[i].imshow(x[i].squeeze(),\n",
        "                       interpolation='none',\n",
        "                       cmap='gray')\n",
        "    axs.flat[i].axis('off')\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JUy_urkfc4m"
      },
      "source": [
        "print('Originals:')\n",
        "display_imgs(x)\n",
        "\n",
        "print('Decoded Random Samples:')\n",
        "display_imgs(xhat.sample())\n",
        "\n",
        "print('Decoded Modes:')\n",
        "display_imgs(xhat.mode())\n",
        "\n",
        "print('Decoded Means:')\n",
        "display_imgs(xhat.mean())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}